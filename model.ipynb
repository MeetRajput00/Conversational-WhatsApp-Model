{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHUlA2ZoSpPQ"
      },
      "outputs": [],
      "source": [
        "## magic commands\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "## import required packages\n",
        "from fastai.text import *\n",
        "from fastai.imports import *\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsoAcmx-S-oH",
        "outputId": "61b96fab-bb28-4530-e840-cf695111244a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "## Colab google drive stuff\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'my_model/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir spacy==2.3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSVUarieWPvq"
      },
      "outputs": [],
      "source": [
        "## prepare data from chat.txt\n",
        "path = Path(base_dir)\n",
        "final_data=[]\n",
        "data=[]\n",
        "with open(path/'chat.txt') as f:\n",
        "  line=''\n",
        "  for char in f.read():\n",
        "    line+=char\n",
        "    if char=='\\n':\n",
        "      try:\n",
        "        timestamp,second_part=line.split('-')\n",
        "        sender,msg=second_part.split(':')\n",
        "        final_data.append([timestamp.strip(),sender.strip(),msg.strip()])\n",
        "      except:\n",
        "        pass\n",
        "      data.append(line)\n",
        "      line=''\n",
        "      continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMQItviYQTwu"
      },
      "outputs": [],
      "source": [
        "#prepare dataframe from chat.txt\n",
        "df = pd.DataFrame(final_data, columns=['timestamp', 'sender', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bJys55kYGzo"
      },
      "outputs": [],
      "source": [
        "#import model requriements\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing\n",
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgZvGYELQVhF",
        "outputId": "cf3301c0-fbe2-43d2-d278-b693352223ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOCAB SIZE : 3938\n"
          ]
        }
      ],
      "source": [
        "#split data from dataframe into questions and answers list\n",
        "questions=list()\n",
        "answers=list()\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  if row['sender']=='Meet Rajput':\n",
        "    answers.append(row['text'])\n",
        "  else:\n",
        "    questions.append(row['text'])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( ' ' + answers_with_tags[i] + ' ' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VltZpSWvYpaK"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tEoTReXYsWl"
      },
      "outputs": [],
      "source": [
        "#tokenize the dataframe\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "  vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "  tokens_list = []\n",
        "  vocabulary = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    tokens = sentence.split()\n",
        "    vocabulary += tokens\n",
        "    tokens_list.append(tokens)\n",
        "  return tokens_list, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK8sfl2zZU_y"
      },
      "outputs": [],
      "source": [
        "#equalize both questions and answers\n",
        "answers=answers[:2991]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PGnhqVhYugn",
        "outputId": "9f9c9c0f-9671-43f6-8b27-7c652bae43ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2991, 39) 39\n"
          ]
        }
      ],
      "source": [
        "#encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "print(encoder_input_data.shape, maxlen_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34i3h5r3YyLu",
        "outputId": "f8c07f8e-72d7-414c-f8c4-e7ece001c3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2991, 14) 14\n"
          ]
        }
      ],
      "source": [
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFHXjyRsY1fF",
        "outputId": "95a3f4af-65be-49d8-883c-e120ead6b676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2991, 14, 3938)\n"
          ]
        }
      ],
      "source": [
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je8MGUjuY4m1",
        "outputId": "6fadc7aa-5a8e-4818-859e-10cfa4a94d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 39)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 39, 200)      787600      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 14, 200)      787600      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 200),        320800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 14, 200),    320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm_1[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_1[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 14, 3938)     791538      ['lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,008,338\n",
            "Trainable params: 3,008,338\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax )\n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WSWBYvGY7I1",
        "outputId": "88977478-a9cb-4837-de1e-0d83373f3004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "60/60 [==============================] - 36s 409ms/step - loss: 6.5486\n",
            "Epoch 2/150\n",
            "60/60 [==============================] - 24s 404ms/step - loss: 4.7160\n",
            "Epoch 3/150\n",
            "60/60 [==============================] - 24s 390ms/step - loss: 4.3372\n",
            "Epoch 4/150\n",
            "60/60 [==============================] - 25s 417ms/step - loss: 4.1593\n",
            "Epoch 5/150\n",
            "60/60 [==============================] - 23s 384ms/step - loss: 4.0584\n",
            "Epoch 6/150\n",
            "60/60 [==============================] - 23s 380ms/step - loss: 4.0054\n",
            "Epoch 7/150\n",
            "60/60 [==============================] - 24s 394ms/step - loss: 3.9412\n",
            "Epoch 8/150\n",
            "60/60 [==============================] - 22s 369ms/step - loss: 3.9025\n",
            "Epoch 9/150\n",
            "60/60 [==============================] - 25s 416ms/step - loss: 3.8596\n",
            "Epoch 10/150\n",
            "60/60 [==============================] - 22s 372ms/step - loss: 3.8272\n",
            "Epoch 11/150\n",
            "60/60 [==============================] - 25s 415ms/step - loss: 3.7975\n",
            "Epoch 12/150\n",
            "60/60 [==============================] - 22s 374ms/step - loss: 3.7700\n",
            "Epoch 13/150\n",
            "60/60 [==============================] - 24s 400ms/step - loss: 3.7477\n",
            "Epoch 14/150\n",
            "60/60 [==============================] - 23s 384ms/step - loss: 3.7162\n",
            "Epoch 15/150\n",
            "60/60 [==============================] - 23s 386ms/step - loss: 3.7050\n",
            "Epoch 16/150\n",
            "60/60 [==============================] - 24s 399ms/step - loss: 3.6794\n",
            "Epoch 17/150\n",
            "60/60 [==============================] - 22s 369ms/step - loss: 3.6611\n",
            "Epoch 18/150\n",
            "60/60 [==============================] - 25s 410ms/step - loss: 3.6405\n",
            "Epoch 19/150\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 3.6230\n",
            "Epoch 20/150\n",
            "60/60 [==============================] - 25s 422ms/step - loss: 3.6023\n",
            "Epoch 21/150\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 3.5904\n",
            "Epoch 22/150\n",
            "60/60 [==============================] - 24s 404ms/step - loss: 3.5656\n",
            "Epoch 23/150\n",
            "60/60 [==============================] - 24s 391ms/step - loss: 3.5613\n",
            "Epoch 24/150\n",
            "60/60 [==============================] - 23s 392ms/step - loss: 3.5367\n",
            "Epoch 25/150\n",
            "60/60 [==============================] - 25s 407ms/step - loss: 3.5217\n",
            "Epoch 26/150\n",
            "60/60 [==============================] - 23s 393ms/step - loss: 3.4906\n",
            "Epoch 27/150\n",
            "60/60 [==============================] - 24s 405ms/step - loss: 3.4697\n",
            "Epoch 28/150\n",
            "60/60 [==============================] - 23s 379ms/step - loss: 3.4552\n",
            "Epoch 29/150\n",
            "60/60 [==============================] - 25s 410ms/step - loss: 3.4375\n",
            "Epoch 30/150\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 3.4347\n",
            "Epoch 31/150\n",
            "60/60 [==============================] - 25s 417ms/step - loss: 3.4110\n",
            "Epoch 32/150\n",
            "60/60 [==============================] - 22s 374ms/step - loss: 3.3963\n",
            "Epoch 33/150\n",
            "60/60 [==============================] - 25s 410ms/step - loss: 3.3753\n",
            "Epoch 34/150\n",
            "60/60 [==============================] - 23s 377ms/step - loss: 3.3602\n",
            "Epoch 35/150\n",
            "60/60 [==============================] - 24s 407ms/step - loss: 3.3430\n",
            "Epoch 36/150\n",
            "60/60 [==============================] - 24s 390ms/step - loss: 3.3233\n",
            "Epoch 37/150\n",
            "60/60 [==============================] - 23s 390ms/step - loss: 3.3118\n",
            "Epoch 38/150\n",
            "60/60 [==============================] - 24s 400ms/step - loss: 3.2953\n",
            "Epoch 39/150\n",
            "60/60 [==============================] - 22s 376ms/step - loss: 3.2824\n",
            "Epoch 40/150\n",
            "60/60 [==============================] - 25s 411ms/step - loss: 3.2554\n",
            "Epoch 41/150\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 3.2480\n",
            "Epoch 42/150\n",
            "60/60 [==============================] - 25s 414ms/step - loss: 3.2301\n",
            "Epoch 43/150\n",
            "60/60 [==============================] - 22s 370ms/step - loss: 3.2250\n",
            "Epoch 44/150\n",
            "60/60 [==============================] - 24s 406ms/step - loss: 3.1994\n",
            "Epoch 45/150\n",
            "60/60 [==============================] - 22s 370ms/step - loss: 3.1842\n",
            "Epoch 46/150\n",
            "60/60 [==============================] - 24s 392ms/step - loss: 3.1688\n",
            "Epoch 47/150\n",
            "60/60 [==============================] - 24s 402ms/step - loss: 3.1636\n",
            "Epoch 48/150\n",
            "60/60 [==============================] - 23s 379ms/step - loss: 3.1387\n",
            "Epoch 49/150\n",
            "60/60 [==============================] - 25s 408ms/step - loss: 3.1198\n",
            "Epoch 50/150\n",
            "60/60 [==============================] - 22s 372ms/step - loss: 3.1098\n",
            "Epoch 51/150\n",
            "60/60 [==============================] - 25s 421ms/step - loss: 3.0883\n",
            "Epoch 52/150\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 3.0887\n",
            "Epoch 53/150\n",
            "60/60 [==============================] - 25s 414ms/step - loss: 3.0618\n",
            "Epoch 54/150\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 3.0489\n",
            "Epoch 55/150\n",
            "60/60 [==============================] - 23s 392ms/step - loss: 3.0346\n",
            "Epoch 56/150\n",
            "60/60 [==============================] - 23s 381ms/step - loss: 3.0254\n",
            "Epoch 57/150\n",
            "60/60 [==============================] - 22s 363ms/step - loss: 3.0039\n",
            "Epoch 58/150\n",
            "60/60 [==============================] - 24s 405ms/step - loss: 2.9969\n",
            "Epoch 59/150\n",
            "60/60 [==============================] - 22s 362ms/step - loss: 2.9789\n",
            "Epoch 60/150\n",
            "60/60 [==============================] - 24s 394ms/step - loss: 2.9614\n",
            "Epoch 61/150\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 2.9450\n",
            "Epoch 62/150\n",
            "60/60 [==============================] - 22s 369ms/step - loss: 2.9287\n",
            "Epoch 63/150\n",
            "60/60 [==============================] - 25s 406ms/step - loss: 2.9067\n",
            "Epoch 64/150\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 2.8942\n",
            "Epoch 65/150\n",
            "60/60 [==============================] - 25s 412ms/step - loss: 2.8722\n",
            "Epoch 66/150\n",
            "60/60 [==============================] - 22s 374ms/step - loss: 2.8522\n",
            "Epoch 67/150\n",
            "60/60 [==============================] - 24s 400ms/step - loss: 2.8367\n",
            "Epoch 68/150\n",
            "60/60 [==============================] - 23s 372ms/step - loss: 2.8210\n",
            "Epoch 69/150\n",
            "60/60 [==============================] - 22s 370ms/step - loss: 2.8035\n",
            "Epoch 70/150\n",
            "60/60 [==============================] - 24s 402ms/step - loss: 2.7860\n",
            "Epoch 71/150\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 2.7705\n",
            "Epoch 72/150\n",
            "60/60 [==============================] - 25s 414ms/step - loss: 2.7506\n",
            "Epoch 73/150\n",
            "60/60 [==============================] - 23s 377ms/step - loss: 2.7324\n",
            "Epoch 74/150\n",
            "60/60 [==============================] - 26s 428ms/step - loss: 2.7101\n",
            "Epoch 75/150\n",
            "60/60 [==============================] - 23s 380ms/step - loss: 2.6912\n",
            "Epoch 76/150\n",
            "60/60 [==============================] - 25s 424ms/step - loss: 2.6724\n",
            "Epoch 77/150\n",
            "60/60 [==============================] - 23s 382ms/step - loss: 2.6562\n",
            "Epoch 78/150\n",
            "60/60 [==============================] - 24s 406ms/step - loss: 2.6372\n",
            "Epoch 79/150\n",
            "60/60 [==============================] - 24s 389ms/step - loss: 2.6133\n",
            "Epoch 80/150\n",
            "60/60 [==============================] - 23s 391ms/step - loss: 2.5951\n",
            "Epoch 81/150\n",
            "60/60 [==============================] - 24s 391ms/step - loss: 2.5764\n",
            "Epoch 82/150\n",
            "60/60 [==============================] - 22s 370ms/step - loss: 2.5527\n",
            "Epoch 83/150\n",
            "60/60 [==============================] - 25s 413ms/step - loss: 2.5399\n",
            "Epoch 84/150\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 2.5157\n",
            "Epoch 85/150\n",
            "60/60 [==============================] - 25s 418ms/step - loss: 2.4998\n",
            "Epoch 86/150\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 2.4730\n",
            "Epoch 87/150\n",
            "60/60 [==============================] - 25s 416ms/step - loss: 2.4597\n",
            "Epoch 88/150\n",
            "60/60 [==============================] - 23s 383ms/step - loss: 2.4397\n",
            "Epoch 89/150\n",
            "60/60 [==============================] - 25s 414ms/step - loss: 2.4144\n",
            "Epoch 90/150\n",
            "60/60 [==============================] - 23s 387ms/step - loss: 2.3901\n",
            "Epoch 91/150\n",
            "60/60 [==============================] - 24s 394ms/step - loss: 2.3767\n",
            "Epoch 92/150\n",
            "60/60 [==============================] - 24s 396ms/step - loss: 2.3522\n",
            "Epoch 93/150\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 2.3335\n",
            "Epoch 94/150\n",
            "60/60 [==============================] - 25s 409ms/step - loss: 2.3134\n",
            "Epoch 95/150\n",
            "60/60 [==============================] - 22s 372ms/step - loss: 2.2958\n",
            "Epoch 96/150\n",
            "60/60 [==============================] - 25s 414ms/step - loss: 2.2693\n",
            "Epoch 97/150\n",
            "60/60 [==============================] - 23s 383ms/step - loss: 2.2492\n",
            "Epoch 98/150\n",
            "60/60 [==============================] - 25s 413ms/step - loss: 2.2307\n",
            "Epoch 99/150\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 2.2127\n",
            "Epoch 100/150\n",
            "60/60 [==============================] - 24s 399ms/step - loss: 2.1849\n",
            "Epoch 101/150\n",
            "60/60 [==============================] - 23s 387ms/step - loss: 2.1669\n",
            "Epoch 102/150\n",
            "60/60 [==============================] - 23s 382ms/step - loss: 2.1468\n",
            "Epoch 103/150\n",
            "60/60 [==============================] - 24s 404ms/step - loss: 2.1214\n",
            "Epoch 104/150\n",
            "60/60 [==============================] - 22s 370ms/step - loss: 2.1078\n",
            "Epoch 105/150\n",
            "60/60 [==============================] - 25s 413ms/step - loss: 2.0830\n",
            "Epoch 106/150\n",
            "60/60 [==============================] - 22s 369ms/step - loss: 2.0600\n",
            "Epoch 107/150\n",
            "60/60 [==============================] - 24s 409ms/step - loss: 2.0386\n",
            "Epoch 108/150\n",
            "60/60 [==============================] - 22s 370ms/step - loss: 2.0222\n",
            "Epoch 109/150\n",
            "60/60 [==============================] - 23s 389ms/step - loss: 1.9964\n",
            "Epoch 110/150\n",
            "60/60 [==============================] - 24s 390ms/step - loss: 1.9736\n",
            "Epoch 111/150\n",
            "60/60 [==============================] - 22s 375ms/step - loss: 1.9594\n",
            "Epoch 112/150\n",
            "60/60 [==============================] - 25s 415ms/step - loss: 1.9355\n",
            "Epoch 113/150\n",
            "60/60 [==============================] - 22s 374ms/step - loss: 1.9165\n",
            "Epoch 114/150\n",
            "60/60 [==============================] - 25s 420ms/step - loss: 1.8947\n",
            "Epoch 115/150\n",
            "60/60 [==============================] - 22s 375ms/step - loss: 1.8744\n",
            "Epoch 116/150\n",
            "60/60 [==============================] - 25s 417ms/step - loss: 1.8533\n",
            "Epoch 117/150\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 1.8335\n",
            "Epoch 118/150\n",
            "60/60 [==============================] - 24s 395ms/step - loss: 1.8114\n",
            "Epoch 119/150\n",
            "60/60 [==============================] - 23s 383ms/step - loss: 1.7895\n",
            "Epoch 120/150\n",
            "60/60 [==============================] - 22s 372ms/step - loss: 1.7719\n",
            "Epoch 121/150\n",
            "60/60 [==============================] - 25s 407ms/step - loss: 1.7431\n",
            "Epoch 122/150\n",
            "60/60 [==============================] - 22s 367ms/step - loss: 1.7291\n",
            "Epoch 123/150\n",
            "60/60 [==============================] - 25s 411ms/step - loss: 1.7086\n",
            "Epoch 124/150\n",
            "60/60 [==============================] - 22s 375ms/step - loss: 1.6889\n",
            "Epoch 125/150\n",
            "60/60 [==============================] - 24s 405ms/step - loss: 1.6669\n",
            "Epoch 126/150\n",
            "60/60 [==============================] - 23s 386ms/step - loss: 1.6497\n",
            "Epoch 127/150\n",
            "60/60 [==============================] - 24s 400ms/step - loss: 1.6311\n",
            "Epoch 128/150\n",
            "60/60 [==============================] - 24s 406ms/step - loss: 1.6051\n",
            "Epoch 129/150\n",
            "60/60 [==============================] - 23s 389ms/step - loss: 1.5827\n",
            "Epoch 130/150\n",
            "60/60 [==============================] - 24s 401ms/step - loss: 1.5650\n",
            "Epoch 131/150\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 1.5521\n",
            "Epoch 132/150\n",
            "60/60 [==============================] - 25s 413ms/step - loss: 1.5259\n",
            "Epoch 133/150\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 1.5068\n",
            "Epoch 134/150\n",
            "60/60 [==============================] - 25s 410ms/step - loss: 1.4901\n",
            "Epoch 135/150\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 1.4706\n",
            "Epoch 136/150\n",
            "60/60 [==============================] - 24s 395ms/step - loss: 1.4506\n",
            "Epoch 137/150\n",
            "60/60 [==============================] - 24s 392ms/step - loss: 1.4363\n",
            "Epoch 138/150\n",
            "60/60 [==============================] - 23s 386ms/step - loss: 1.4101\n",
            "Epoch 139/150\n",
            "60/60 [==============================] - 24s 403ms/step - loss: 1.3960\n",
            "Epoch 140/150\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 1.3729\n",
            "Epoch 141/150\n",
            "60/60 [==============================] - 25s 416ms/step - loss: 1.3525\n",
            "Epoch 142/150\n",
            "60/60 [==============================] - 23s 377ms/step - loss: 1.3364\n",
            "Epoch 143/150\n",
            "60/60 [==============================] - 25s 421ms/step - loss: 1.3150\n",
            "Epoch 144/150\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 1.2967\n",
            "Epoch 145/150\n",
            "60/60 [==============================] - 24s 398ms/step - loss: 1.2810\n",
            "Epoch 146/150\n",
            "60/60 [==============================] - 23s 377ms/step - loss: 1.2600\n",
            "Epoch 147/150\n",
            "60/60 [==============================] - 22s 375ms/step - loss: 1.2433\n",
            "Epoch 148/150\n",
            "60/60 [==============================] - 24s 404ms/step - loss: 1.2237\n",
            "Epoch 149/150\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 1.2083\n",
            "Epoch 150/150\n",
            "60/60 [==============================] - 25s 414ms/step - loss: 1.1926\n"
          ]
        }
      ],
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 )\n",
        "model.save( 'model.h5' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdKDx1MhZBll"
      },
      "outputs": [],
      "source": [
        "def make_inference_models():\n",
        "\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "\n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbeCkXOrnJH5"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] )\n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1pLPpHZhnLmw",
        "outputId": "f6b6f997-c39b-4516-b1f3-7fa9c9d9e6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter question : Tera set konsa h\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            " test ka h karne me karne ki karne ki karne ki karne ki karne ki\n",
            "Enter question : Aaj kya hoga\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            " ho karne me karne ki karne ki karne ki karne ki karne ki karne ki\n",
            "Enter question : One piece AOT naruto kede lge the bhot\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            " karne karne karne karne karne karne karne karne karne karne karne karne karne karne karne\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-a3576c66748e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'Enter question : '\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ]\n",
        "\n",
        "    print( decoded_translation )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
